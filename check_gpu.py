#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
GPU/CPUè®¾å¤‡æ£€æµ‹å’Œå¯è§†åŒ–è„šæœ¬
"""

import torch
import sys

def print_separator(char="=", length=60):
    """æ‰“å°åˆ†éš”çº¿"""
    print(char * length)

def visualize_device_info():
    """å¯è§†åŒ–è®¾å¤‡ä¿¡æ¯"""
    print_separator()
    print("è®¾å¤‡ä¿¡æ¯æ£€æµ‹")
    print_separator()
    
    # PyTorchç‰ˆæœ¬
    print(f"\nğŸ“¦ PyTorchç‰ˆæœ¬: {torch.__version__}")
    
    # CUDAä¿¡æ¯
    print(f"\nğŸ” CUDAæ£€æµ‹:")
    cuda_available = torch.cuda.is_available()
    print(f"  CUDAæ˜¯å¦å¯ç”¨: {'âœ“ æ˜¯' if cuda_available else 'âœ— å¦'}")
    
    if cuda_available:
        print(f"  CUDAç‰ˆæœ¬: {torch.version.cuda}")
        print(f"  cuDNNç‰ˆæœ¬: {torch.backends.cudnn.version()}")
        print(f"  cuDNNå¯ç”¨: {torch.backends.cudnn.enabled}")
        
        # GPUä¿¡æ¯
        print(f"\nğŸ® GPUä¿¡æ¯:")
        gpu_count = torch.cuda.device_count()
        print(f"  GPUæ•°é‡: {gpu_count}")
        
        for i in range(gpu_count):
            print(f"\n  GPU {i}:")
            props = torch.cuda.get_device_properties(i)
            print(f"    åç§°: {torch.cuda.get_device_name(i)}")
            print(f"    è®¡ç®—èƒ½åŠ›: {props.major}.{props.minor}")
            print(f"    æ€»å†…å­˜: {props.total_memory / 1024**3:.2f} GB")
            print(f"    å¤šå¤„ç†å™¨æ•°é‡: {props.multi_processor_count}")
            
            # å½“å‰å†…å­˜ä½¿ç”¨
            if i == torch.cuda.current_device():
                allocated = torch.cuda.memory_allocated(i) / 1024**2
                reserved = torch.cuda.memory_reserved(i) / 1024**2
                print(f"    å½“å‰å†…å­˜ä½¿ç”¨:")
                print(f"      å·²åˆ†é…: {allocated:.2f} MB")
                print(f"      å·²ä¿ç•™: {reserved:.2f} MB")
        
        # æ¨èé…ç½®
        print(f"\nğŸ’¡ æ¨èè®¾ç½®:")
        print(f"  å½“å‰è®¾å¤‡: GPU {torch.cuda.current_device()}")
        print(f"  å»ºè®®æ‰¹æ¬¡å¤§å°: æ ¹æ®GPUå†…å­˜è°ƒæ•´")
        
        # æµ‹è¯•GPUè®¡ç®—
        print(f"\nğŸ§ª GPUè®¡ç®—æµ‹è¯•:")
        try:
            x = torch.randn(1000, 1000).cuda()
            y = torch.randn(1000, 1000).cuda()
            z = torch.matmul(x, y)
            print(f"  çŸ©é˜µä¹˜æ³•æµ‹è¯•: âœ“ æˆåŠŸ")
            print(f"  è®¡ç®—è®¾å¤‡: {z.device}")
        except Exception as e:
            print(f"  GPUè®¡ç®—æµ‹è¯•å¤±è´¥: {e}")
        
    else:
        print(f"\nâš ï¸  è­¦å‘Š: æœªæ£€æµ‹åˆ°GPU")
        print(f"  å°†ä½¿ç”¨CPUè¿›è¡Œè®¡ç®—")
        print(f"  å»ºè®®:")
        print(f"    - å¦‚æœæ‚¨æœ‰GPUï¼Œè¯·å®‰è£…GPUç‰ˆæœ¬çš„PyTorch")
        print(f"    - å®‰è£…å‘½ä»¤: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118")
        
        # CPUä¿¡æ¯
        print(f"\nğŸ’» CPUä¿¡æ¯:")
        print(f"  å¯ç”¨æ ¸å¿ƒæ•°: {torch.get_num_threads()}")
    
    # æœ€ç»ˆå»ºè®®
    print_separator()
    print("è®­ç»ƒå»ºè®®")
    print_separator()
    
    if cuda_available:
        print("âœ“ æ£€æµ‹åˆ°GPUï¼Œå¯ä»¥å¼€å§‹GPUè®­ç»ƒ")
        print("  è¿è¡Œå‘½ä»¤:")
        print("    python scripts/precompute_protein_embeddings.py")
        print("    python scripts/train.py")
    else:
        print("âš  æœªæ£€æµ‹åˆ°GPUï¼Œå°†ä½¿ç”¨CPUè®­ç»ƒ")
        print("  CPUè®­ç»ƒé€Ÿåº¦è¾ƒæ…¢ï¼Œå»ºè®®ï¼š")
        print("    1. ä½¿ç”¨GPUæœåŠ¡å™¨æˆ–äº‘æœåŠ¡")
        print("    2. å‡å°æ‰¹æ¬¡å¤§å°å’Œæ•°æ®é‡è¿›è¡Œæµ‹è¯•")
    
    print_separator()

if __name__ == '__main__':
    visualize_device_info()
